* [x] U-net iseg2017 过拟合
* [x] 尝试其他常用于脑组织分割的模型
* [x] 完成9-23例会ppt
* [x] 综述：
* [x] 1. 找到一篇文章框架
* [x] 2. 找一些自己的内容往里面填充替换
* [x] 3. 修改论文
  * [x] 1. 扩充损失函数
  * [x] 2. 修改hd公式
  * [x] 3. 完善“进一步的工作”

---

- [x] 1.完成一个基于边缘注意力的实验(DNL)，不一定要有提升，画图，写到方案里【周二周三2天】
  - [x] 想办法把DNL加到Unet里
  - [x] 画图
- [x] 2.完成边缘注意力方案，周六提交【周四全力1天】
  - [x] 研究目标
  - [x] ~内容
  - [x] 关键问题
  - [x] ~方法（具体方案）-> 根据实验来写
- [ ] 3.其他实验
  - [x] 查看之前Unet5（边缘注意力）的attention map，是否真正关注到了边缘
  - [ ] 完成两阶段(snake)实验

---

### 切入点

canny找边界 再边界注意力（通道注意力 空间注意力）

想办法使边界保持连续，不要出现孤立点

数据输入的时候，怎么处理t1和t2，一起放，还是单独放，还是其他，t1指导t2还是t2指导t1之类的

跳线连接，传导信息的方式

rnn 考虑前后切片之间的关系

相邻切片之间会有大变化吗，不大考虑加入约束

为什么要做这个 背景
为什么要用这种方法 现状



半监督方法



可能的切入点：

1. 2d方法中想办法处理好边缘（脑脊液、灰质和白质的交界线），因为分错的基本都在边缘部分
2. 想办法提取、利用空间特征，因为现在效果好（iSeg2017）的方法大多都是3d方法，充分利用了空间特征，但参数量太大



attention unet论文中提到，用FCN，RNN来引入相邻切片间约束。

---

Idea：

1. 在unet跳线中间加att模块，拼过去的就是注意力图，而不是原来的特征图了

2. 改善卷积模块，dense-att：在最开始加att模块，然后将注意图链接到每次卷积前（融合到每次卷积的输入）

3. 引入**迁移学习**，将*U-Net的下采样部分*用*预训练模型的特征提取器*替换。

   何凯明的《Rethinking ImageNet Pre-training》中指出：

   > 数据量足够的情况下，迁移训练不会影响（提高）最终的性能，但是能够加速收敛

   婴幼儿的数据量少，因此我认为引入迁移学习是有意义的。
   
4. 多轴面/空间transformer

---

目标：

1. 达到一个能接受的performance，网站测试集上：csf94，gm92，wm91
2. 证明添加的模块有效果（消融实验）

方向：

1. **空间信息**很重要，多个实验都表明只要引入了**多轴面**，performance就提升**2个点**

   1. 多轴面
   2. 相邻切片约束
   3. 3D卷积

2. 注意力

   如果使用自注意力，那是否应输入整张图像，而不是(64, 64, 64)的块

   将注意力和空间信息进行结合
   
   1. 多轴面注意力（不太好证明是注意力的效果，说明2D的注意力根本就没有效果）
   2. 3D卷积的注意力模块
   3. 相邻多切片注意力（已有类似研究）
   4. 空间transformers
   5. 时序(RNN)注意力，将Batch当作时序 [Recurrent Models of Visual Attention](https://papers.nips.cc/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf) 
   
3. 提点方法

   - 残差链接
   - 以下出自[TransUNet](https://github.com/Beckschen/TransUNet/blob/main/networks/vit_seg_modeling_resnet_skip.py)代码

     - 权值归一化的卷积（**stdconv**）
     - 组归一化Group Normalization（**GN**）替代BN
     - 激活函数Gaussian Error Linerar Units(**GELUs**)替代ReLU
     - 激活函数**Swish**替代ReLU（[激活函数ref](https://www.cnblogs.com/makefile/p/activation-function.html)）

   - 迁移学习

---

2021/6/22整理

改进点：

- [ ] 1. 双卷积改为resblock，bottleneck
- [ ] 2. 卷积改为**stdconv**
- [ ] 3. Layerlstm的**残差**连接
- [ ] 4. **激活**函数（swish？）
- [ ] 5. 引入部分**3D**卷积
- [ ] 6. 减少层数（减少BCDLSTM）
- [ ] 7. 减少BCDLSTM的长度，提高**并行**性
- [ ] 8. **WN**权重正则化
- [ ] 9. 多复用特征，dense？

可能性较低的点：

- [ ] 1. 多/单**模态**如何输入，在哪里输入
- [ ] 2. **预处理**方法
- [ ] 3. 引入**门控**在融合前筛选特征
- [ ] 4. Light CNN

难实现的点：

- [ ] 1. 通过**置信度**（预测概率的绝对值大小）进一步优化结果
- [ ] 2. **损失**函数（在1的基础上进一步定制损失函数？）