# 记录方法

1. 摘要 **Abstract**

   1. 问题
   2. 方法
   3. 结论：作者给出的结论是什么

2. 引言 **Introduction**

   相对没那么重要，通常是对摘要进行详细扩充，

   1. 动机/背景：为什么要研究这个问题
   2. 现状
   3. 难点/挑战
   4. 方法
   5. 先验知识：引用的其他人的观点、方法

3. 结论 **Conclusion**

   1. 缺陷、不足
   2. 创新点、贡献

# Deep convolutional neural networks for brain image analysis on magnetic resonance imaging: a review

1. 摘要

   MRI中用到的CNN（深度学习）技术，关注这些工作中的网络结构，预处理，数据准备以及后处理。

   主要讲各个CNN架构的发展，讨论SOTA，分析优缺点。

   展望，未来的研究方向。

# The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks

1. 摘要

   Jaccard index又叫IoU，通常用作评价分割结果

   虽然能作为评价指标，但由于IoU是离散函数，无法求导，故无法用作模型训练的损失函数

   方法：提出了一种直接优化**平均IoU损失**的方法，基于子模式损失的凸lovasz extension

   结论：实验证明，在IoU评价指标上，该损失比传统交叉熵损失的性能更好（训练时优化得更好？）

   > 一句话概括就是：采用lovasz extension这个数学工具将离散的Jaccard loss (eq. 4/6)变得光滑化(eq. 8-9)，从而可以直接求导。
   >
   > from [知乎](https://zhuanlan.zhihu.com/p/76825379)

   >???
   >
   >We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset.

2. 引言

   多数深度网络分割算法依赖于**逻辑回归**（logistic regression）和**交叉熵损失**（cross-entropy loss）

   交叉熵损失**并不能很好地评价图像分割的质量**，而Jaccard index（又叫IoU）是一个更好的选择

   $$
   J(A,B)=\frac{|A\cap{B}|}{|A\cup{B}|}
   $$
   对于**多分类**任务，Jaccard index通常取各个类IoU的均值，即mIoU

   提出了一个分段线性的凸函数来代替不可求导的Jaccard loss

   从数学上来说，该代替函数是基于**子模块集函数的lovasz 扩展**（based on the Lovasz
   extension of submodular set functions）

   然后介绍了现有方法及其存在的问题

3. 总结

   展示了一种用于优化Jaccard loss以进行图像分割的通用方法，使用方便（可作为一个独立模块，即插即用）

   展示了该方法在SOTA的深度模型上的性能，即：只是选用了更好的loss，就能提升模型准确率（在语义分割数据集上的准确率）

   定性上，在小物体上的分割质量大大提高；在大物体上具有一致性（不会导致性能降低？），准确率有小提升

4. [代码](https://github.com/bermanmaxim/LovaszSoftmax)

   可以先用交叉熵训练，再用该loss微调；或者同时使用交叉熵和该loss

   > you might have best results by optimizing with cross-entropy first and finetuning with our loss, or by combining the two losses.
   >
   > from [github](https://github.com/bermanmaxim/LovaszSoftmax#faq)

# A survey of loss functions for semantic segmentation
1. 摘要

   总结了一些著名的用于图像分割的损失函数，说明了适用于哪些情况

   提出了新的log-cosh dice loss，在NBDS头骨分割数据集上进行了实验，并与前面介绍的损失函数做了比较

   某些损失函数在所有数据集上都能很好地发挥作用

   可以作为一个baseline用在数据分布未知的数据集中

2. 引言

   深度学习的发展，使医学等领域从中获益

   介绍了图像分割的定义，以及损失函数在深度学习中的重要性

   总结了15种（包括自己提出的）用于分割的损失函数，这些函数已被证明是各个领域的首选

   这些损失函数分为4类：基于分布，基于区域，基于边界，基于合成（？）

   ![image-20201027203523984](C:\Users\Zhniing\AppData\Roaming\Typora\typora-user-images\image-20201027203523984.png)

   讨论了确定哪种目标/损失函数在场景中可能有用的条件

   提出了新的log-cosh dice loss用于语义分割

3. 结论

   损失函数很重要，但对于分割这种复杂目标来说，没有一个通吃的损失函数。选择损失函数要看数据集的分布，偏斜度（skewness，类别不均匀？），边界等

   比如，在高度imbalanced的分割任务中，focus based（比如Focal loss及其变体）的损失函数效果更好

   Binary Cross-Entropy适合balanced data-sets

   skewed data-sets可以使用smoothed or generalized dice coefficient

   提出了dice loss的变体，使其更易于处理，以实现更好的优化，达到更精确的结果
   
4. [代码](https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions)

# Segmentation Loss Odyssey

1. 摘要

   医学图像分割领域

   虽然目前已经提出很多loss，但大多是单独或少数放在一起研究（没有一个全面的综述）

   提出了一个分类方法，分为4个类别，有助于找到他们的联系，衍生图如下（这是github上的图，比论文中的更新）

   探索了传统的基于**区域**和最近的基于**边界**的loss的关系

   代码用pytorch实现

   ![LossOverview.PNG](https://github.com/JunMa11/SegLoss/blob/master/test/LossOverview.PNG?raw=true)

2. 引言

   根据优化目标进行分类

   有4类目标：使**分布差异、区域差异、边界差异、某种组合差异**最小化

   将会探索这些loss之间的联系

3. 结论

4. [代码](https://github.com/JunMa11/SegLoss)

# Deformable Convolutional Networks

`2020-9-28`

> Dai J, Qi H, Xiong Y, et al. Deformable convolutional networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 764-773.

创新：**卷积网络的新思路**。传统CNN卷积核的几何形状是固定的，因此会带来一些局限性，本文提出了**可学习**的**可变形**卷积核和池化区域，即卷积核和池化区域的形状都是可以改变并且可以学习的。使卷积核和池化区域能提取更有用的信息、关注更有用的区域。

解决问题：传统卷积核的几何局限性

应用场景：目标检测，语义分割

疑问：卷积过程中特征图尺度不变？

源代码：<https://github.com/msracver/Deformable-ConvNets>

参考博客：<https://blog.csdn.net/sunshine_010/article/details/80219473>的参考文献

# InfiNet: Fully convolutional networks for infant brain MRI segmentation

1. 摘要

   1. 问题：婴儿大脑等强度期分割

   2. 方法：InfiNet，两个编码器分别输入t1和t2，再送入一个联合解码器，根据下采样时的池化索引，来执行非线性的、不需要学习的上采样操作。

      > 没看懂：
      >
      > The sparse maps are concatenated with intermediate encoder representations (skip connections) and convolved with trainable filters to produce dense feature maps.

   3. 成果：在50秒内完成一个实例的（全体积）分割，展示比一些SOTA方法更好的性能

2. 引言

   1. 背景：婴幼儿脑分割的研究价值
   2. 现状：大多为成人，因为年龄导致解剖学上的差异，使得方法迁移的效果不理想。针对等强度期的方法大多不是端到端，或者分割速度慢
   3. 挑战：低信噪比（Signal-to-Noise ratio，SNR），运动伪影，灰白质间对比度低，强度？不均
   4. 方法：基于FCN的方法流行起来了，能进行端到端分割。本文提出了一种FCN的变体，InfiNet。两条编码路径分别处理两个模态的图像。对三个轴面的图像都做这样的处理，最后再将它们融合起来

3. 方法

   1. 用两条编码路径分别提取两个模态（T1w和T2w）的特征
   2. 保存池化索引做反池化（与SegNet的上采样方式相同）
   3. GDL（Generalized Dice Loss）带权损失，权重为：类别频率平方的倒数
   4. 加入空间特征：三个独立的网络处理三个轴面，进行切片级的分割（即分割的是2D图像），最后按**平均值**融合，得到结果

4. 总结

# MMAN: Multi-modality aggregation network for brain segmentation from MR images

2019

1. 摘要

   问题：手动的脑组织分割存在time-consuming，tedious，subjective（主观）的问题

   难点：边界不明确，解剖学上的复杂结构，个体差异（这些难点导致一些通用的方法效果不好）

   提出：多模态聚合网络（multi-modality aggregation network，MMAN）

   创新：综合**多尺度**，**多模态**的信息进行更快更准的分割

   数据：MRBrainS

   性能：灰质86.40%，白质89.70%，脑脊液84.86%，MRBrainS第二名，13秒分割一个脑（其余前10名中最快2min）

2. 引言

   介绍了背景，数据集（MRBrainS），通过传统方法引出深度方法，每种相关的深度方法都用一两句话简要介绍，指出现有研究的盲点（没有侧重于多尺度的研究）
   
3. 方法

   DIB（Dilated-Inception block），结合了**空洞卷积**和**Inception结构**

   - 空洞卷积：用于获得**更大的感受野**，并且不会增加参数量和计算成本

   - Inception结构：用于将各个特征进行**融合**？

   - 通过**拼接（concatenation）**将**不同空洞率的3个并列空洞卷积**的输出结合（combine）起来，再用**1x1卷积**将拼起来的特征聚合（aggregate）起来

   整体架构类似FCN，而不是U-Net；与FCN的不同：（**改进点**）
   
   - 下采样阶段有3条路径对应3个模态的输入
   - 下采样阶段就是把普通卷积换成了DIB
   - 上采样之前先对三个模态的特征进行了融合（拼接和DIB）
   
   整体来看也只是在下采样阶段做了文章：下采样阶段扩展为3条路经，然后在进入上采样阶段前，进行特征融合（拼接+DIB）。这些改动**对上采样阶段透明**。

# Going Deeper with Convolutions

1. 摘要

   提出：一种新的网络结构，代号*Inception*

   数据：ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)

   创新：计算资源的**利用率**得到提高，在不增加计算成本的条件下，增加网络的深度和宽度（在有限的计算资源内，进一步提升网络性能）

   性能：

2. Inception结构

   

[博客](https://blog.csdn.net/weixin_39953502/article/details/80966046)总结：

在 Inception 之前, 大多数流行的 CNN 只是将卷积层堆叠得越来越深，以期获得更好的效果。

提升网络性能最直白的方法就是增加网络的深度（层数）和宽度（各层神经元数）

问题：

- 不同图像中目标所占空间的大小各不相同，造成难以选择合适的卷积核大小，当信息分布更全局时，倾向选择一个较大的核；当信息分布的更局部时，倾向选择一个较小的核。
- 网络过深容易过拟合，容易梯度消失，很难将梯度更新传至整个网络（此时还没有提出BN，优化很困难）
- 单纯堆叠卷积会造成计算成本的大幅上升

创新：在同一层上采用不同尺寸的多个卷积核，网络将变得**更宽**，而不是更深

GoogLeNet中使用了9个Inception模块。由于网络还是**太深**，为避免中间层发生**梯度消失**，引入了2个辅助分类器（仅用于训练，推理时将被忽略），每经过3个Inception模块，就输出一个分类器，最终损失是各分类器损失的**加权和**

[译文](https://blog.csdn.net/huangfei711/article/details/70336079)

# MixNet: Multi-modality Mix Network for Brain Segmentation
1. 摘要

   提出：MixNet，**2D**的语义级分割网络，多模态MRI输入

   创新：残差学习单元（residual learning units）

# Densely Connected Convolutional Networks

DenseNet脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维

与其多次学习冗余的特征,特征复用是一种更好的特征提取方式

# Non-local Neural Networks

[cnblogs](https://www.cnblogs.com/pprp/p/12199807.html)

**传统卷积**和**滤波算子**都只计算一个局部区域（如3x3）

而**非局部**均值滤波的操作是：计算**所有像素**的**加权平均**，即计算目标和图中其他像素的相关性，得到的结果表征其他位置和当前位置的相似度，可以认为用了一个跟原图一样大的卷积核来卷积

然而实际计算时，如果真的计算所有像素，成本太高，所以实际都是计算block和block的相关性

non-local操作的输出跟原图大小一致

non-local操作可以看作是attention

该方法从传统方法Non local means中获得灵感。

融合了图像的全局信息（但脑组织分割的全局信息有什么意义吗？整张图的边界**模式**似乎都是差不多的？）

文中通过**消融实验**证明了：该模块在视频分类、目标检测、**实例分割**、关键点检测等领域的**有效性**

# Non-Local U-Net

Title: Non-local U-Nets for Biomedical Image Segmentation

整个网络采用**3D**卷积

Input Block用的文中图2a，Output Block在此基础上增加了1x1x1卷积

Down-sampling Block用的文中图2b，用卷积代替池化

Bottom Block用的文中图2c，是一个包含Global Aggregation Block的残差结构

Up-sampling Block用的文中图2d，用反卷积作为上采样手段

# Disentangled Non-Local

Title: Disentangled Non-Local Neural Networks

1. 摘要

   背景：non-local block通常用来加强网络的**上下文建模能力**（the context modeling ability）

   提出：the **disentangled** non-local block

   解决：whitened pairwise term和unary term两个阶段高度耦合，导致训练不好的问题。将2个阶段分离开来，解决耦合问题

   场景：语义分割，目标检测，动作识别

作者指出：描述两像素点相关性的**二元函数**包含一个一元函数(**unary**)和一个二元函数(**pairwise**)

一元项unary：描述边缘（**边缘**信息）。表示一个像素对**所有**像素的影响（？）

二元项pairwise：描述区域（**类别**信息）。表示一个像素对**另一个**像素的影响（？）

文中用**点积（=内积=数量积）**度量相似性，**疑问**：2D图像如何计算点积？

[知乎](https://zhuanlan.zhihu.com/p/148737297)启发性地解释了Query、Key和Value三者的关系，抽象地讲解了目前注意力机制的计算过程，**疑问**：最后加权得到的Attention Value是一个值（标量），如何将其引入图像领域？

为什么要**解耦**：原来相**乘**的融合方式中，如果一项趋于0，则另一项也起不到任何作用。解耦改为相**加**的融合方式

白化（**Whitened**）：[博客](https://blog.csdn.net/hjimce/article/details/50864602)

1. 作用：**降低输入数据的冗余性**。图像中相邻像素的相关性很强，这些强相关的像素都是冗余的（？）。白化通过降低特征之间的相关性，从而降低数据的冗余度
2. 算法：与PCA相似，只是不进行降维，具体分为PCA白化、ZCA白化

# RGB-like
Title: From neonatal to adult brain MR image segmentation in a few seconds using 3D-like fully convolutional network and transfer learning

提出：结合**迁移学习**的全卷积网络，快速分割3D大脑组织

创新：将连续3张2D切片拼接成一组，形成类似RGB三通道的图像，即**RGB-like**（原文为：3D-like）图像

贡献：

1. 快！（因为不是全3D网络，而是）3D-like的方法
2. 适用婴儿和成人
3. 由于迁移学习，不依赖大量医学图像

方法：

1. 预处理：直方图均衡化
2. 网络模型：backbone是vgg-16，在ImageNet上预训练，4个池化把网络分为5个卷积阶段，每个阶段结束后添加3x3的特殊卷积层（Specialized Layers）， 结果**resize**到原图大小，再**拼接**起来，再接一个1x1的卷积（对各层结果进行线性组合），得到最终结果。输入为连续三张切片（边界补0），输出为中间切片的预测结果。损失函数：multinomial logistic loss。数据增强：缩放和旋转。每个像素减去所在通道的均值（每张切片减去自己的均值）。带有momentum的SGD(momentum=0.99，weight_decay=0.005)。每20张图像进行一次损失函数平均
3. 后处理：没看懂

# 2D+3D/2.5D/supervised attention

Title: Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss

提到了医学影像术语：**各项异性**([anisotropic](https://www.zhihu.com/question/382284292))分辨率，表示xyz三个扫描方向的采样分辨率不同

贡献：

1. 第一个使用深度学习的VS肿瘤分割方法，一种结合2D和3D卷积的**2.5D**分割方法（用于应对平面间的低分辨率）
2. 提出一种**注意力模块**
3. 提出一种**voxel-level Hardness-weighted Dice Loss**

方法：

1. 2.5D的网络结构：U-net的5层结构，前2层2D，后3层3D（因为**面内**分辨率是**面间**分辨率的4倍，因此经过2次下采样后，达到**近似各向同性**的分辨率）。激活函数：pReLU(含参的relu)。
2. 多尺度 监督 空间注意力：注意力图是一个单通道图像，内容为每个像素的注意力系数（代表每个位置的相对重要性）。模块结构：卷积(通道减半)->relu->卷积(输出单通道)->sigmoid->注意力图作为权重乘到原图上。使用了残差连接。加入了注意力损失用来监督注意力模块的学习，使网络更关注**小的目标区域**。
3. 损失函数**voxel-level Hardness-weighted Dice Loss**：dice loss的改进，加入像素级的权重，困难样本的权重更高。

提到了**配对t检验**([paired t-test](https://zhuanlan.zhihu.com/p/30900167))：用于检测同一事物，在处理前后的差异

# Attention U-Net
Title: Attention U-Net: Learning Where to Look for the Pancreas

[论文解析](https://towardsdatascience.com/a-detailed-explanation-of-the-attention-u-net-b371a5590831)

注意力分类：

1. hard attention

   通过裁剪crop或迭代地区域建议，得到一个相关区域，因此不可微分，需要*增强学习*来训练。

   由于**不可微分**，反向传播也就不起效，也就不知道注意力有没有起作用。

2. soft attention

   原理：给图像的不同部分**加权**，相关性越高，权重越大。

   可以微分并直接训练。并且**权重**也是训练出来的。

创新点：**Attention Gate**

# UNet++

分为[会议版](https://link.springer.com/chapter/10.1007/978-3-030-00889-5_1)和[期刊版](https://ieeexplore.ieee.org/document/8932614)，作者的[知乎文章](https://zhuanlan.zhihu.com/p/44958351)

**解读**多尺度：不同层次的特征，或者说不同大小的感受野，对于大小不一的目标对象的敏感度是不同的，比如，感受野大的特征，可以很容易的识别出大物体的，但是在实际分割中，大物体边缘信息和小物体本身是很容易被深层网络一次次的降采样和一次次升采样给弄丢的，这个时候就可能需要感受野小的特征来帮助。

**剪枝**：对于比较难的数据集，可以看到网络越深，它的分割结果是在不断上升的。对于大多数比较**简单**的分割问题，其实并不需要非常深，非常大的网络就可以达到很不错的精度了。

# Transformer

[知乎](https://zhuanlan.zhihu.com/p/48508221)

**Transformer**来自Google 2018年提出的BERT算法，在NLP领域取得了*新高*

Transformer抛弃了CNN和RNN，整个网络由Attention组成

Transformer由且仅由**self-Attention**和**Feed Forward** Neural Network组成

网络通过堆叠Transformer进行搭建

encoder-decoder结构

RNN有2个缺陷：

1. 每次计算依赖上一步的结果，限制了并行能力，不符合现有GPU架构
2. 顺序计算过程中会丢失信息（？

Transformer解决了上述两个问题

讲解Transformer的[英文博客](http://jalammar.github.io/illustrated-transformer/)，用*机器翻译*的例子讲解的

### 1. Transformer

由多个encoder和decoder组成， 每个encoder包含：

1. Self-Attention模块：（带残差连接，从模块*输入连到输出*
   1. 通过一个公式的得到注意力（加权向量），与non-local类似（就是nonlocal？
2. Feed Forward模块：（带残差链接
   1. 全连接层+Relu
   2. 全连接层+线性激活（=没有激活函数？）

**Q**uery，**K**ey，**V**alue的概念来自*信息检索系统*

使用**点积**计算两个矩阵的相似度：QK（矩阵点积可以衡量相似度吗？

### 2. Multi-Head Attention（也带残差连接

多个不同的（权值不同）的self attention模块**并行**计算，最后将得到的结果拼接融合起来

### 3. Encoder-Decoder Attention模块

Decoder比Encoder多了一个模块，在self attention和feed forward之间插入了一个模块：**encoder-decoder attention**

计算方法与self attention相同，其中Q来自上一个模块（multi-head attention），K和V来自encoder