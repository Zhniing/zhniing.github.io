# 注意力机制

### 分类

- 按原理：

  **软注意力(soft attention)**：**可微**，权重可通过训练学到，可作为一个模块嵌入已有网络中

  **硬注意力(hard attention)**：**不可微**，需要采用强化学习进行训练

- 按作用域：

  **空间域(spatial domain)**：2015年 [STN](https://papers.nips.cc/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf)(Spatial Transformer Networks)(pytorch1.7.1已集成)

  **通道域(channel domain)**：2018年 [SENet](https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf)(Squeeze-and-Excitation Networks)

  **混合域(mixed domain)**：2017年 [Residual Attention Network for Image Classification](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Residual_Attention_Network_CVPR_2017_paper.pdf)

  **时间域(time domain)**：2014年 [Recurrent Models of Visual Attention](https://papers.nips.cc/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf) 基于RNN的注意力机制
  
- 自注意力：2018年 Non-Local

端到端：**sequence to sequence**：seq2seq

## Vision Transformer

报告《[Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet](https://github.com/lukemelas/do-you-even-need-attention/blob/main/Do-You-Even-Need-Attention.pdf)》指出：ViT的效果好，不全是因为注意力模块。报告将注意力替换为全连接层(feed-forward layer)后，发现性能几乎不受影响。
