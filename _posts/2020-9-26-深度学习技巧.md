---
layout: post
title: 深度学习技巧
categories: [深度学习]
---

# 深度学习技巧

## 解决过拟合

1. 权值衰减(weight decay): Adam优化器自带参数
2. 学习率衰减
3. Dropout
4. Batch Normalization: 与Relu一起使用, 防止Dead Relu, 也能加速训练?
5. 增加训练集的样本数

### 调整学习率

* 动手设定多少轮后，学习率为多少

* 随着迭代次数增加，学习率自动衰减，Pytorch提供了一些常用的衰减类：

  1. 指数衰减 ExponentialLR
     $$
     lr = lr_{base} * \gamma ^ {epoch}
     \tag {1}
     $$
     
  2. 等步长衰减 StepLR

     每隔一定epoch，就衰减为原来的γ（gamma）倍（0<γ<1）

     比如step_size=30，则在epoch为30、60、90时进行调整

     $$
     lr = lr_{base} * \gamma ^ {\lfloor \frac{epoch}{stepsize} \rfloor}
     $$

  3. 多步长衰减 MultiStepLR

     达到milestones=[200, 300, 320, 340, 400]指定的步数时，学习率衰减为γ倍
     
     ---
     
     **小结**：其实前面三种方法本质上都是按epoch进行衰减，只是发生衰减的步数不同
     
     - 指数衰减就是每个epoch都衰减一次
     - 等步长就是间隔固定的步长后衰减一次
     - 多步长就是自定义在第几个epoch进行衰减
     
     ---
     
  4. 余弦退火衰减 CosineAnnealingLR

     学习率周期变化，而不是衰减

  5. 自适应调整学习率 ReduceLROnPlateau

     > 当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略

  6. 自定义调整学习率 LambdaLR

     > 为不同参数组设定不同学习率调整策略，fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略

* 在每个epoch结束时调用返回的scheduler对象的step()方法

  `scheduler.step()`

* 查看当前学习率

  `scheduler.get_lr()`

  可以通过查看每个类对get_lr()的实现方法，来找到每个类的学习率调整公式

* 示例

  ```python
  import torch.optim as optim
  
  optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=0.0005)
  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.01)
  
  for epoch in range(100):
  	scheduler.step()
  	# train(...)
  	# validate(...)
  ```
  
> [参考博客](https://blog.csdn.net/shanglianlm/article/details/85143614)
>
> [官方文档](https://pytorch.org/docs/1.2.0/optim.html#how-to-adjust-learning-rate)

### Dropout

> 一般适合于全连接层，卷积层由于参数不多，所以不是很需要Dropout，加上也对模型泛化能力没有太大影响。

干的事情：

* Pytorch的Dropout设置的p是失活率（drop rate）
* 不是按失活比例进行失活，而是对每个神经元按失活率进行失活（对每个输出点按失活率进行失活）。因此，**实际失活的数量不一定等于失活率的比例**

实验：

* Pytorch的Dropout分为：
  * Dropout
  * Dropout2d
  * Dropout3d
* 不加dropout时，训练集效果最好，p越高，训练集效果越差
* 在p（失活率）为0.1~0.8的实验中，p=0.1的训练、验证结果最好
* p为0.6时，训练集性能下降明显（验证集也一样），p为0.7和0.8时，就几乎学不动了，训练和验证的Dice很早就收敛到0.5左右

总结：

* drop rate越高训练集上的效果越差，感觉像是强行降低在训练集上的拟合能力来提高泛化能力。。。
* 确实能够抑制一点过拟合，但是dice依然抖动严重，可能还需要加上学习率衰减。