---
layout: post
title: 深度学习技巧
categories: [深度学习]
---

# 深度学习技巧

## 过拟合解决方法:

1. 权值衰减(weight decay): Adam优化器自带参数
2. 学习率衰减
3. Dropout
4. Batch Normalization: 与Relu一起使用, 防止Dead Relu, 也能加速训练?
5. 增加训练集的样本数

### 学习率衰减方法：

* 动手设定多少轮后，学习率为多少

* 随着迭代次数增加，学习率自动衰减

  * 指数衰减

  decayed_learning_rate = \
  learning_rate * **decay_rate^(global_steps/decay_steps)**

  * 固定步长衰减：每隔一定步数（epoch），就衰减为原来的γ（gamma）倍（0<γ<1）
  * 多步长衰减：达到milestones=[200, 300, 320, 340, 400]指定的步数时，学习率衰减为γ倍
  
  * 余弦退火衰减：学习率周期变化，而不是衰减
  
* 上述方法对应的4个类：ExponentialLR、StepLR、MultiStepLR和CosineAnnealingLR，在每个epoch结束时调用返回的scheduler对象的step()方法

  `scheduler.step()`

### Dropout

* Pytorch Dropout设置的p是失活率（drop rate）
* 不是按失活比例进行失活，而是对每个神经元按失活率进行失活（对每个输出点按失活率进行失活）。因此，**实际失活的数量不一定等于失活率的比例**
* drop rate越高训练集上的效果越差，感觉像是强行降低在训练集上的拟合能力来提高泛化能力。。
* 确实能够抑制一点过拟合，但是dice依然抖动严重，可能还需要加上学习率衰减。



